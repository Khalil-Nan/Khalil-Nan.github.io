---
title: '深度学习笔记第三章（李沐加李宏毅的课程）'
date: 2026-02-08
permalink: /posts/2026/02/深度学习笔记第三章（李沐加李宏毅的课程）
tags:
    - 深度学习
---

# 深度学习笔记第三章（李沐加李宏毅的课程）

## 第 3 章 深度学习基础

#### 3.1 局部极小值与鞍点

##### 3.1.1 临界点及其种类

不知有局部最小点，还有鞍点，两者都称为临界点

##### 3.1.2 判断临界值种类的方法

![](C:\Users\Dazzle\AppData\Roaming\marktext\images\2026-02-11-17-36-49-image.png)

这里是泰勒展开，H是海森矩阵，g是梯度，就是L对θ的每一个元素求导组成的列向量，当是临界点的时候梯度等于0所以就剩下最后一项了，所以如果这个海森矩阵是正定矩阵的话就是$\theta'$就是最小值，负定就是最大值，否则就是鞍点。    ，这里判断是否正定只需要看它所有的特征值是不是都是正的。

<mark>似乎有点忘记了正定矩阵，等概念是相对对称矩阵而言的？</mark>

如果对于鞍点而言如何逃出鞍点呢？海森矩阵也告诉了我们方案：就是说当θ-θ’等于负特征值的特征向量的时候这个方向就是误差函数减少的方向，实际情况由于海森矩阵的计算量巨大，所以一般不采用这种方法计算，而是用别的方法逃离鞍点

关于鞍点和局部极值点出现概率的大小问题，由于维度特别高，所以局部极小值的情况是很难出现的。

#### 3.2 批量和动量

计算损失及梯度，更新参数，不是在整个训练集上进行的，而是在一个batch进行的，全部数据过一遍叫一个epoch，当然batch是需要shuffle的，    常见的做法是每一个epoch都shuffle一下。

##### 3.2.1 批量大小对梯度下降法的影响

<mark>随机梯度下降的梯度上引入了随机噪声,因此在非凸优化问题中,其相比批量梯度下  降更容易逃离局部最小值。</mark>

首先一个问题是批量大小对计算速度的影响，实际上对应于1000条数据和1条数据的计算时间由于是GPU并行计算，差距并没有那么大，但是批量大到一定程度还是有影响的，所以说对于批量过小的情况要花的时间，因为更新次数多反而花费时间长。

- 大的批量结果稳定，考虑并行运算的话有时候反而会比较快

- 小的批量有噪声，可以帮助训练

为什么不同批量的损失函数不一样，因为损失函数是使用batch的数据来算的，求导变量是参数，所以不同批量数据不一样，损失函数也不一样。

我这里理解到一个问题就是参数的更新是只利用求梯度的一个方向的，不利用大小的。

即使对于大批量和小批量能学习得一样好，在测试集上小批量结果更好，一个可能的解释，就是说局部最小值也有不同，一个峡谷的最小值，和一个平原的最小值，而对于小批量来说更容易跳出峡谷的局部最小值，对于峡谷的局部最小值来说，当换到测试集后，损失函数计算出来的损失就很大。<mark>大的批量大小会让我们倾向于走到“峡谷”里面</mark>这里还是理解不够深刻，为什么测试集的损失函数对于训练集出来的参数，有类似平移的效果，怎么理解。

##### 3.2.2 动量法

参考的是真实世界的惯性的概念，本质就是更新的时候不仅减去梯度乘以学习率，还要考虑上一步的移动量乘以$\lambda$

### 3.3 自适应学习率

训练网络时关注梯度的范式，可以check是不是确实降到临界点了，所谓的范式就是梯度这个向量长度，不同的学习率对训练的影响，学习率太大在山谷两边振荡

对于

## 待办

有必要对MNIST进行一下学习体会一下深度学习的整个流程，有个感性的认识
