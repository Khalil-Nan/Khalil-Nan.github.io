---
title: '深度学习笔记（李沐加李宏毅的课程）'
date: 2026-02-08
permalink: /posts/2026/02/深度学习笔记（李沐加李宏毅的课程）
tags:
    - 深度学习
---

# 深度学习笔记第一二章（李沐加李宏毅的课程）

- 有问题的地方都以下划线的形式标出

3.线性神经网络

主要是讲线性回归和softmax回归

回归与预测问题的辨析

<u>线性回归的基本假设：有待加深理解</u>

对于一个样本来说，w是权重的列向量

x是特征的列向量结果就是
$$\hat y=W^Tx$$
而对于矩阵来说每一行是一个样本，每一列是1个特征
$$\hat y=Xw+b$$
应该是一个n行的向量,这里的b使用广播机制

- 模型质量的度量方式
- 提高模型质量的方法

这里用的误差函数是平方误差

![image-20260122191818846](C:\Users\Dazzle\AppData\Roaming\Typora\typora-user-images\image-20260122191818846.png)

注意误差函数是均值，要除以n的

<u>解析解</u>

![image-20260122192120948](C:\Users\Dazzle\AppData\Roaming\Typora\typora-user-images\image-20260122192120948.png)

3.1.1.4随机梯度下降

![image-20260122194816247](C:\Users\Dazzle\AppData\Roaming\Typora\typora-user-images\image-20260122194816247.png)

这里的问题就是

![image-20260122195017545](C:\Users\Dazzle\AppData\Roaming\Typora\typora-user-images\image-20260122195017545.png)

上面这个东西对W求导，也就是
$$
W^TX对W求导为什么是x.
$$
<u>为什么要使用小batch随机梯度下降的原因？</u>

和李宏毅似乎有分歧

批量大小和学习率是新加的两个超参数

之前分训练集，测试集和验证集的目的是干嘛来着？

## 李宏毅的书

有一些任务中 *y* 和 *y*ˆ 都是概率分布，这个时候可能会选择**交叉熵（****cross entropy****）**，这

机器学习的三个步骤

1. 写出带有w和b参数的要找的函数
2. 定义误差函数
3. 解一个最优化的问题
   1. 这里就涉及两个问题一个是关于斜率的问题另外一个就是关于学习率的问题

一个常见的方法是梯度下降算法

梯度下降算法的一个问题就是局部最优解不是全局最优解

很漂亮的一个地方在于利用领域知识对模型进行一个修改，比如说因为7天一个周期，那把7天的数据投喂进去会不会比较好的预测，首先的效果是在训练集上就能得到更低的loss，然后更多的天数比如说1个月可能效果又会更好

有时候线性模型的效果又上限，可以用常数加上一堆的hard sigmoid函数就可以形成分段线性曲线，hard sigmoid 函数不好写所以用sigmoid函数逼近，

## 1.2.1 分段线性曲线

- 首先是一个特征的此情况下可以有不同的sigmoid的函数叠加来逼近非线性函数
  
  $y = b+\sum_{i} c_iσ(b_i + w_ix1)$这里的b很有意思因为当x趋于负无穷的时候，sigmoid函数是等于零的所以需要这个偏置项

- 这里的构建更有灵活性的函数也比较有意思就是说针对一个特征的时候，是用sigmoid函数来毕竟任意函数，而对于多个特征来说并不是直接的对i求和再对j求和，不是直接符合逻辑的做法而是在i个sigmoid的函数基础上原先wx1变成了多个特征的积的和这里的逻辑基本上是按照下面这张图来的，我的想法应该也没问题，不过就是9个sigmoid函数了
  
  ![](C:\Users\Dazzle\AppData\Roaming\marktext\images\2026-02-08-17-37-50-image.png)

batch：批量

epoch:把所有的batch都看过一遍叫做一个epoch

example

**超参数**

sigmoid的个数（我的理解就是神经元的个数），学习率，批量大小

## 1.2.2 模型变形

Rectified Linear Unit,ReLU修正线性单元：$c ∗ max(0, b + wx_1)$

两个RELU就可以表示一个sigmoid，而且似乎ReLU拟合的效果要更好，我的理解是更灵活，不像sigmoid的函数的两边满足一定约束条件

这里引入了层数这个超参数也就是神经网络有几层，不过最后一层肯定是像上面那个图最后一样。

这里引入了过拟合的概念

## 1.2.3 机器学习框架

## 第 2 章 实践方法论

### 在训练集上学不好的原因

#### 2.1 模型偏差

模型太简单

解决方案：

1. 深度学习

2. 更多特征

3. 可以增加激活函数也就是神经元的个数

#### 2.2 优化问题

局部最优解的问题

怎么分辨局部最优解的问题还是模型偏差的问题？

我的思路是多训练几次，如果有些降下来，有些不降，那就是局部最优解的问题，如果咋样都降不下来那就是模型偏差问题

- 想法是针对不同大小的模型进行训练如果比较深的模型的训练集损失还大，说明是优化方法不给力，因为深层网络一定能干到浅层网络的效果。

- 方法论就是先训练一些浅的网络，**支持向量机**比较不会有优化的问题，这样就能清楚误差的范围，然后再训练深的网络，如果误差反而还大就是优化的问题。

#### 2.3 过拟合

1. 增加数据/数据增强，就像旋转照片一样

2. 限制灵活性
   
   1. 较少的参数：例子就是cnn比全连接网络要限制大，但在图像上效果好
   2. 比较少的特征
   3. 别的方法：正则化，dropout等等

一个很有意思的情况就是你假如说用三个模型去测试集上测，哪个好就用哪个的化会出现一个情况，就是即使是一个模型的作用是随机输出，只要模型足够多，也可以在测试集上取得很好的效果，但这种模型在私人数据集上就毫无用处。

### 2.4 交叉验证

就是训练数据分为训练集和验证集，然后公开测试集的结果就能反映私人测试集的结果，但是这个循环太多次，也就过拟合了，真的是有点哲学的味道在里面。

为了解决选的训练集和验证集没有代表性，会采用一个方法：k 折交叉验证(k-fold  cross validation)，就是训练集分成假设份，然后任意两份训练一份验证，最后取平均这样来选模型，选完模型再全部用到训练集上。

### 2.5 不匹配

是测试集和训练集的分布不同，这个匹不匹配的问题涉及到对数据的理解。
